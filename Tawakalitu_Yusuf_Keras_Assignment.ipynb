{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMy98fWCUBl8/1jXrLNf2Wn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tawaqalt/arbritrary/blob/master/Tawakalitu_Yusuf_Keras_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Problem 1] Sharing and executing the official tutorial model"
      ],
      "metadata": {
        "id": "wYvudrYplxV3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0trJmd6DjqBZ",
        "outputId": "51fec92d-8088-473a-e342-922bcf22336f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.15.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(\"TensorFlow version:\", tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7FP5258xjs-v"
      },
      "outputs": [],
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPZ68wASog_I"
      },
      "source": [
        "## Build a machine learning model\n",
        "\n",
        "Build a `tf.keras.Sequential` model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "h3IKyzTCDNGo"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(10)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "OeOrNdnkEEcR",
        "outputId": "e8925410-9c0c-4b3f-8b03-123091fadbe6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.24265102,  0.07631977,  0.6078176 ,  0.44360787,  0.01114468,\n",
              "        -0.13607776,  0.16177276, -0.36387512, -0.00415626, -0.29822028]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "predictions = model(x_train[:1]).numpy()\n",
        "predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgjhDQGcIniO"
      },
      "source": [
        "The `tf.nn.softmax` function converts these logits to *probabilities* for each class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zWSRnQ0WI5eq",
        "outputId": "ace2182c-8802-420b-cb91-1cc99e220964",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.07297325, 0.10039011, 0.17081167, 0.14494464, 0.09405585,\n",
              "        0.08117978, 0.10934596, 0.0646423 , 0.09262765, 0.06902879]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "tf.nn.softmax(predictions).numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQyugpgRIyrA"
      },
      "source": [
        "Define a loss function for training using `losses.SparseCategoricalCrossentropy`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "RSkzdv8MD0tT"
      },
      "outputs": [],
      "source": [
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfR4MsSDU880"
      },
      "source": [
        "The loss function takes a vector of ground truth values and a vector of logits and returns a scalar loss for each example. This loss is equal to the negative log probability of the true class: The loss is zero if the model is sure of the correct class.\n",
        "\n",
        "This untrained model gives probabilities close to random (1/10 for each class), so the initial loss should be close to `-tf.math.log(1/10) ~= 2.3`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "NJWqEVrrJ7ZB",
        "outputId": "1754e3fc-3159-44fd-8f74-80e714c97c7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.511089"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "loss_fn(y_train[:1], predictions).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "9foNKHzTD2Vo"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=loss_fn,\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "F7dTAzgHDUh7",
        "outputId": "1dccec7f-ca3a-419b-b191-d04260afebd4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 - 1s - loss: 2.3218 - accuracy: 0.1121 - 779ms/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2.3218331336975098, 0.11209999769926071]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "model.evaluate(x_test,  y_test, verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "rYb6DrEH0GMv"
      },
      "outputs": [],
      "source": [
        "probability_model = tf.keras.Sequential([\n",
        "  model,\n",
        "  tf.keras.layers.Softmax()\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "cnqOZtUp1YR_",
        "outputId": "389db6f2-55b2-46b3-dc5a-7fa2588c563c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5, 10), dtype=float32, numpy=\n",
              "array([[0.11813828, 0.07464056, 0.2127665 , 0.07140718, 0.08145025,\n",
              "        0.08858814, 0.11122575, 0.07887568, 0.07673828, 0.08616927],\n",
              "       [0.17598452, 0.07657045, 0.12640083, 0.0771206 , 0.12244269,\n",
              "        0.08520081, 0.14741817, 0.07357255, 0.07413699, 0.04115234],\n",
              "       [0.08159023, 0.09088317, 0.18980698, 0.09151282, 0.08231628,\n",
              "        0.09612679, 0.09407604, 0.08749816, 0.09946989, 0.08671959],\n",
              "       [0.10655832, 0.0496406 , 0.1487284 , 0.09938994, 0.11070856,\n",
              "        0.05091699, 0.1369828 , 0.15430926, 0.10123526, 0.04152979],\n",
              "       [0.12079419, 0.07077374, 0.2031578 , 0.09014641, 0.12876342,\n",
              "        0.08910011, 0.10749895, 0.06340913, 0.06857519, 0.05778109]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "probability_model(x_test[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Problem 3] Learning Iris (binary classification) with Keras"
      ],
      "metadata": {
        "id": "F_TRvfSRxU5R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Loading the Iris dataset\n",
        "iris = pd.read_csv('Iris.csv')\n",
        "\n",
        "# Filter for Iris-versicolor and Iris-virginica\n",
        "iris_binary = iris[(iris['Species'] == 'Iris-versicolor') | (iris['Species'] == 'Iris-virginica')]\n",
        "\n",
        "# Extract features and labels\n",
        "X = iris_binary.drop('Species', axis=1)\n",
        "y = iris_binary['Species']\n",
        "\n",
        "# Encode labels (0 for Iris-versicolor, 1 for Iris-virginica)\n",
        "y = np.where(y == 'Iris-versicolor', 0, 1)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train_binary, X_test_binary, y_train_binary, y_test_binary = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardizing the features\n",
        "scaler = StandardScaler()\n",
        "X_train_binary = scaler.fit_transform(X_train_binary)\n",
        "X_test_binary = scaler.transform(X_test_binary)\n",
        "\n",
        "# Defining the model\n",
        "model_binary = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(10, activation='relu', input_shape=(X_train_binary.shape[1],)),\n",
        "    tf.keras.layers.Dense(10, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compiling the model\n",
        "loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
        "model_binary.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model_binary.fit(X_train_binary, y_train_binary, epochs=5, batch_size=5, validation_split=0.2)\n",
        "\n",
        "# Evaluation of the model\n",
        "loss, accuracy = model_binary.evaluate(X_test_binary, y_test_binary)\n",
        "print(f'Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}')\n",
        "\n",
        "# Making predictions\n",
        "predictions_binary = model_binary.predict(X_test_binary)\n",
        "predicted_classes_binary = (predictions_binary > 0.5).astype(int)\n",
        "print(predicted_classes_binary.flatten())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsnTj0Z5xUeH",
        "outputId": "299dceda-75d9-4a01-a807-5dc12ebe4ab0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "13/13 [==============================] - 3s 81ms/step - loss: 0.6641 - accuracy: 0.6250 - val_loss: 0.6360 - val_accuracy: 0.8125\n",
            "Epoch 2/5\n",
            "13/13 [==============================] - 0s 11ms/step - loss: 0.6145 - accuracy: 0.8438 - val_loss: 0.6064 - val_accuracy: 0.9375\n",
            "Epoch 3/5\n",
            "13/13 [==============================] - 0s 14ms/step - loss: 0.5751 - accuracy: 0.8594 - val_loss: 0.5785 - val_accuracy: 0.8750\n",
            "Epoch 4/5\n",
            "13/13 [==============================] - 0s 7ms/step - loss: 0.5395 - accuracy: 0.8906 - val_loss: 0.5517 - val_accuracy: 0.8750\n",
            "Epoch 5/5\n",
            "13/13 [==============================] - 0s 8ms/step - loss: 0.5075 - accuracy: 0.9219 - val_loss: 0.5237 - val_accuracy: 0.8750\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.4865 - accuracy: 0.9500\n",
            "Test Loss: 0.4865, Test Accuracy: 0.9500\n",
            "1/1 [==============================] - 0s 181ms/step\n",
            "[1 1 1 0 0 0 0 1 0 1 0 0 1 0 1 0 1 1 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Problem 4] Learning Iris (multi-level classification) with Keras"
      ],
      "metadata": {
        "id": "LCYEUxi40fKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Iris dataset\n",
        "iris = pd.read_csv('Iris.csv')\n",
        "\n",
        "# Extracting features and labels\n",
        "X = iris.drop('Species', axis=1)\n",
        "y = iris['Species']\n",
        "\n",
        "# One-hot encoding the labels\n",
        "encoder = OneHotEncoder()\n",
        "y_encoded = encoder.fit_transform(y.values.reshape(-1, 1)).toarray()\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train_multi, X_test_multi, y_train_multi, y_test_multi = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardizing the features\n",
        "scaler = StandardScaler()\n",
        "X_train_multi = scaler.fit_transform(X_train_multi)\n",
        "X_test_multi = scaler.transform(X_test_multi)\n",
        "\n",
        "# Defining the model\n",
        "model_multi = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(10, activation='relu', input_shape=(X_train_multi.shape[1],)),\n",
        "    tf.keras.layers.Dense(10, activation='relu'),\n",
        "    tf.keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compiling the model\n",
        "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
        "model_multi.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])\n",
        "\n",
        "# Training the model\n",
        "model_multi.fit(X_train_multi, y_train_multi, epochs=10, batch_size=5, validation_split=0.2)\n",
        "\n",
        "# Evaluating the model\n",
        "loss, accuracy = model_multi.evaluate(X_test_multi, y_test_multi)\n",
        "print(f'\\nTest Loss: {loss:.4f}, \\nTest Accuracy: {accuracy:.4f}')\n",
        "\n",
        "# Make predictions\n",
        "predictions_multi = model_multi.predict(X_test_multi)\n",
        "predicted_classes_multi = np.argmax(predictions_multi, axis=1)\n",
        "print(predicted_classes_multi)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ec8KOm-F0qUC",
        "outputId": "8f5b8394-f564-40bc-ef50-1b579f5a8b98"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "20/20 [==============================] - 2s 25ms/step - loss: 1.0422 - accuracy: 0.4375 - val_loss: 1.0125 - val_accuracy: 0.6250\n",
            "Epoch 2/10\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.9849 - accuracy: 0.5312 - val_loss: 0.9724 - val_accuracy: 0.5833\n",
            "Epoch 3/10\n",
            "20/20 [==============================] - 0s 9ms/step - loss: 0.9390 - accuracy: 0.5729 - val_loss: 0.9302 - val_accuracy: 0.6250\n",
            "Epoch 4/10\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.8928 - accuracy: 0.6042 - val_loss: 0.8987 - val_accuracy: 0.6667\n",
            "Epoch 5/10\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.8469 - accuracy: 0.6354 - val_loss: 0.8579 - val_accuracy: 0.7500\n",
            "Epoch 6/10\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 0.7975 - accuracy: 0.6875 - val_loss: 0.8155 - val_accuracy: 0.7500\n",
            "Epoch 7/10\n",
            "20/20 [==============================] - 0s 13ms/step - loss: 0.7481 - accuracy: 0.6771 - val_loss: 0.7637 - val_accuracy: 0.7500\n",
            "Epoch 8/10\n",
            "20/20 [==============================] - 0s 8ms/step - loss: 0.6971 - accuracy: 0.7292 - val_loss: 0.7260 - val_accuracy: 0.7917\n",
            "Epoch 9/10\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.6490 - accuracy: 0.7708 - val_loss: 0.6850 - val_accuracy: 0.7917\n",
            "Epoch 10/10\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.6044 - accuracy: 0.8021 - val_loss: 0.6390 - val_accuracy: 0.7917\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.5426 - accuracy: 0.8667\n",
            "\n",
            "Test Loss: 0.5426, \n",
            "Test Accuracy: 0.8667\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "[1 0 2 2 2 0 1 2 1 1 2 0 0 0 0 2 2 1 1 2 0 2 0 2 2 2 1 2 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Problem 5] Learning House Prices with Keras"
      ],
      "metadata": {
        "id": "FpGksF9K05b8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "\n",
        "# Loading the House Prices dataset\n",
        "house_prices = pd.read_csv('train.csv')\n",
        "\n",
        "# Extracting features and labels\n",
        "X = house_prices[['GrLivArea', 'YearBuilt']]\n",
        "y = house_prices['SalePrice']\n",
        "\n",
        "# Splitting into training and testing sets\n",
        "X_train_hp, X_test_hp, y_train_hp, y_test_hp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardizing the features\n",
        "scaler = StandardScaler()\n",
        "X_train_hp = scaler.fit_transform(X_train_hp)\n",
        "X_test_hp = scaler.transform(X_test_hp)\n",
        "\n",
        "# Defining the model\n",
        "model_hp = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train_hp.shape[1],)),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(1)  # Output layer for regression\n",
        "])\n",
        "\n",
        "# Compiling the model\n",
        "loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "model_hp.compile(optimizer='adam', loss=loss_fn, metrics=['MeanAbsoluteError'])\n",
        "\n",
        "# Training the model\n",
        "model_hp.fit(X_train_hp, y_train_hp, epochs=5, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluating the model\n",
        "loss, mae = model_hp.evaluate(X_test_hp, y_test_hp)\n",
        "print(f'\\nTest Mean Absolute Error: ${mae:.2f}')\n",
        "\n",
        "# Making predictions using the model\n",
        "predictions_hp = model_hp.predict(X_test_hp)\n",
        "print(predictions_hp.flatten())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkuVwIqL00Rb",
        "outputId": "a782acfd-bfd9-4c4c-c826-c6bd5c2624fa"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "30/30 [==============================] - 3s 16ms/step - loss: 39146835968.0000 - mean_absolute_error: 181526.1875 - val_loss: 37841817600.0000 - val_mean_absolute_error: 181101.2656\n",
            "Epoch 2/5\n",
            "30/30 [==============================] - 0s 12ms/step - loss: 39146242048.0000 - mean_absolute_error: 181524.6875 - val_loss: 37840961536.0000 - val_mean_absolute_error: 181099.1406\n",
            "Epoch 3/5\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 39144935424.0000 - mean_absolute_error: 181521.3906 - val_loss: 37839130624.0000 - val_mean_absolute_error: 181094.5469\n",
            "Epoch 4/5\n",
            "30/30 [==============================] - 0s 13ms/step - loss: 39142330368.0000 - mean_absolute_error: 181514.9062 - val_loss: 37835784192.0000 - val_mean_absolute_error: 181086.1719\n",
            "Epoch 5/5\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 39137796096.0000 - mean_absolute_error: 181503.6094 - val_loss: 37830123520.0000 - val_mean_absolute_error: 181072.1094\n",
            "10/10 [==============================] - 0s 8ms/step - loss: 39641477120.0000 - mean_absolute_error: 178809.1250\n",
            "\n",
            "Test Mean Absolute Error: $178809.12\n",
            "10/10 [==============================] - 0s 4ms/step\n",
            "[ 20.389345  60.30508   30.830523  21.462275  36.158417  27.70813\n",
            "  26.172562  21.10215   27.995678  18.756218  27.780636  23.323856\n",
            "  30.084421  32.556793  34.07714   23.638716  36.99678   19.968292\n",
            "  33.456837  30.889427  22.87295   36.02265   31.938599  24.207827\n",
            "  27.998955  23.813324  30.898804  26.619045  30.330612  30.531033\n",
            "  36.544937  45.84468   59.938545  23.908873  35.97733   18.932392\n",
            "  24.449738  42.40078   56.420135  25.217503  27.034304  43.445564\n",
            "  21.54316   48.174034  22.170563  45.129803  23.135715  21.980433\n",
            "  55.58717   18.314379  24.725899  22.428528  18.787434  63.46548\n",
            "  27.421024  37.89747   33.076454  18.776287  18.705599  26.50003\n",
            "  38.162975  18.834648  59.242638  34.255825  39.532288  30.374918\n",
            "  26.758774  52.148758  23.015055  28.390055  22.386961  17.641724\n",
            "  24.537064  23.944614  58.061005  25.293402  53.72941   52.996323\n",
            "  19.994038  24.737606  20.750393  32.920197  23.725212  29.412865\n",
            "  20.358791  25.075571  44.189484  22.962788  27.421024  32.131508\n",
            "  20.587261  21.547121  23.693916  61.09798   23.538975  29.615105\n",
            "  28.83216   28.300762  32.92756   45.905025  22.266695  37.783913\n",
            "  38.946304  32.30559   20.783566  18.155218  20.290726  28.463203\n",
            "  23.923866  32.660583  31.254856  19.798763  21.04686   30.732588\n",
            "  40.107582  30.16948   25.28824   23.050194  29.98127   34.14427\n",
            "  26.274338  27.421024  30.234917  29.654362  23.089441  22.528755\n",
            "  50.56234   30.949375  24.295307  23.65667   33.592968  46.297783\n",
            "  35.606033  34.79877   34.660393  38.02179   38.732353  21.497568\n",
            "  34.207146 119.38422   66.59847   24.357409  28.188723  28.692307\n",
            "  22.410332  22.93917   27.978233  28.991657  24.969456  37.01326\n",
            "  31.47109   17.327065  43.623642  36.306522  32.177696  25.46685\n",
            "  17.23686   19.137634  38.41236   19.57845   29.827557  19.777458\n",
            "  36.78639   18.33739   22.5065    23.19729   40.116714  63.82213\n",
            "  72.89935   36.051727  42.867752  25.217503  28.483776  22.892847\n",
            "  68.182236  19.31056   21.538261  29.994164  24.265184  31.184029\n",
            "  17.482862  23.36604   21.195028  21.433084  43.691887  30.170498\n",
            "  35.117302  32.96863   30.226566  19.044554  18.418758  32.58197\n",
            "  23.871082  18.825375  29.517342  26.356682  29.008654  31.477507\n",
            "  34.961098  30.960922  21.54814   26.73759   31.350101  62.08909\n",
            "  28.849861  21.35321   41.25212   19.498947  23.257435  23.245548\n",
            "  45.942173  18.923326  19.050207  28.608652  19.213541  35.50892\n",
            "  31.239592  20.081886  18.21289   25.146658  22.362328  30.201706\n",
            "  23.62116   25.263737  32.882374  19.422852  32.567467  31.472523\n",
            "  21.106586  40.131554  26.657206  26.190517  24.989597  19.811228\n",
            "  36.74785   33.075108  29.623905  23.280888  39.225563  21.746645\n",
            "  34.563995  63.219643  32.160126  75.92413   27.727968  35.276295\n",
            "  17.743484  21.572994  19.437307  28.660362  40.776558  24.929272\n",
            "  23.306355  37.19589   18.261196  19.352585  29.636414  17.842768\n",
            "  27.84121   43.321243  45.181793  18.60751   22.253515  27.548742\n",
            "  38.775986  44.50359   26.521109  27.11507   23.200806  28.488306\n",
            "  48.18791   24.340012  44.634197  18.754564  18.361769  20.628586\n",
            "  36.710403  42.658066  19.294086  21.532904  37.512695  27.995678\n",
            "  34.578697  34.630802  38.576527  19.514462  38.85401   26.476336\n",
            "  33.021904  31.584435  23.298964  23.294876]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Problem 6] Learning MNIST with Keras"
      ],
      "metadata": {
        "id": "0orYQi-15exC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# Loading the MNIST dataset\n",
        "(X_train_mnist, y_train_mnist), (X_test_mnist, y_test_mnist) = mnist.load_data()\n",
        "\n",
        "# Normalizing the images\n",
        "X_train_mnist = X_train_mnist / 255.0\n",
        "X_test_mnist = X_test_mnist / 255.0\n",
        "\n",
        "# Reshaping the images\n",
        "X_train_mnist = X_train_mnist.reshape(X_train_mnist.shape[0], 28, 28, 1)\n",
        "X_test_mnist = X_test_mnist.reshape(X_test_mnist.shape[0], 28, 28, 1)\n",
        "\n",
        "# Defining the model\n",
        "model_mnist = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(10)  # Output layer for 10 classes\n",
        "])\n",
        "\n",
        "# Compiling the model\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model_mnist.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])\n",
        "\n",
        "# Training the model\n",
        "model_mnist.fit(X_train_mnist, y_train_mnist, epochs=5, batch_size=32, validation_data=(X_test_mnist, y_test_mnist))\n",
        "\n",
        "# Evaluating the model\n",
        "loss, accuracy = model_mnist.evaluate(X_test_mnist, y_test_mnist)\n",
        "print(f'\\nTest Loss: {loss:.4f}, \\nTest Accuracy: {accuracy:.4f}')\n",
        "\n",
        "# Make predictions using the model\n",
        "predictions_mnist = model_mnist.predict(X_test_mnist)\n",
        "predicted_classes_mnist = np.argmax(predictions_mnist, axis=1)\n",
        "print(predicted_classes_mnist)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_OpZMO51czb",
        "outputId": "01e966bb-3015-4588-f8a9-ce6130e0c0f5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 66s 34ms/step - loss: 0.1498 - accuracy: 0.9534 - val_loss: 0.0390 - val_accuracy: 0.9875\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 57s 30ms/step - loss: 0.0453 - accuracy: 0.9858 - val_loss: 0.0331 - val_accuracy: 0.9901\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 57s 30ms/step - loss: 0.0341 - accuracy: 0.9890 - val_loss: 0.0316 - val_accuracy: 0.9895\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 56s 30ms/step - loss: 0.0254 - accuracy: 0.9917 - val_loss: 0.0287 - val_accuracy: 0.9913\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 55s 29ms/step - loss: 0.0208 - accuracy: 0.9932 - val_loss: 0.0296 - val_accuracy: 0.9911\n",
            "313/313 [==============================] - 3s 8ms/step - loss: 0.0296 - accuracy: 0.9911\n",
            "\n",
            "Test Loss: 0.0296, \n",
            "Test Accuracy: 0.9911\n",
            "313/313 [==============================] - 4s 12ms/step\n",
            "[7 2 1 ... 4 5 6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Problem 7] Rewriting to PyTorch\n",
        "## 1. Iris Binary Classification with PyTorch"
      ],
      "metadata": {
        "id": "AnbXA7aE7El-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = pd.read_csv('Iris.csv')\n",
        "\n",
        "# Filter for Iris-versicolor and Iris-virginica\n",
        "iris_binary = iris[(iris['Species'] == 'Iris-versicolor') | (iris['Species'] == 'Iris-virginica')]\n",
        "\n",
        "# Extract features and labels\n",
        "X = iris_binary.drop('Species', axis=1).values.astype(np.float32)\n",
        "y = np.where(iris_binary['Species'] == 'Iris-versicolor', 0, 1).astype(np.float32)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train_binary, X_test_binary, y_train_binary, y_test_binary = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_binary = scaler.fit_transform(X_train_binary)\n",
        "X_test_binary = scaler.transform(X_test_binary)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.from_numpy(X_train_binary)\n",
        "y_train_tensor = torch.from_numpy(y_train_binary).view(-1, 1)\n",
        "X_test_tensor = torch.from_numpy(X_test_binary)\n",
        "y_test_tensor = torch.from_numpy(y_test_binary).view(-1, 1)\n",
        "\n",
        "# Define the model\n",
        "class IrisBinaryModel(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(IrisBinaryModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 10)\n",
        "        self.fc2 = nn.Linear(10, 10)\n",
        "        self.fc3 = nn.Linear(10, 1)  # Output layer for binary classification\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.sigmoid(self.fc3(x))\n",
        "        return x\n",
        "\n",
        "# Get the number of features\n",
        "input_dim = X_train_tensor.shape[1]\n",
        "\n",
        "# Instantiate the model, define loss function and optimizer\n",
        "model_binary = IrisBinaryModel(input_dim)\n",
        "loss_fn = nn.BCELoss()\n",
        "optimizer = optim.Adam(model_binary.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    model_binary.train()\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model_binary(X_train_tensor)\n",
        "    loss = loss_fn(outputs, y_train_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluate the model\n",
        "model_binary.eval()\n",
        "with torch.no_grad():\n",
        "    test_outputs = model_binary(X_test_tensor)\n",
        "    predicted_classes_binary = (test_outputs > 0.5).int()\n",
        "    accuracy_binary = (predicted_classes_binary == y_test_tensor).float().mean()\n",
        "    print(f'Test Accuracy: {accuracy_binary:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxoZDaecH5Yd",
        "outputId": "f4ed6eb8-d33f-42a1-b387-24308390c9d5"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/100], Loss: 0.6804\n",
            "Epoch [20/100], Loss: 0.6609\n",
            "Epoch [30/100], Loss: 0.6385\n",
            "Epoch [40/100], Loss: 0.6123\n",
            "Epoch [50/100], Loss: 0.5824\n",
            "Epoch [60/100], Loss: 0.5486\n",
            "Epoch [70/100], Loss: 0.5108\n",
            "Epoch [80/100], Loss: 0.4698\n",
            "Epoch [90/100], Loss: 0.4268\n",
            "Epoch [100/100], Loss: 0.3829\n",
            "Test Accuracy: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pytorch for multiclassification\n"
      ],
      "metadata": {
        "id": "A__P0wDvJeGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Extract features and labels\n",
        "X = iris.drop('Species', axis=1).values.astype(np.float32)\n",
        "y = iris['Species'].values\n",
        "\n",
        "# One-hot encode the labels\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "y_encoded = encoder.fit_transform(y.reshape(-1, 1))\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train_multi, X_test_multi, y_train_multi, y_test_multi = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_multi = scaler.fit_transform(X_train_multi)\n",
        "X_test_multi = scaler.transform(X_test_multi)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_tensor_multi = torch.from_numpy(X_train_multi)\n",
        "y_train_tensor_multi = torch.from_numpy(y_train_multi).float()\n",
        "X_test_tensor_multi = torch.from_numpy(X_test_multi)\n",
        "y_test_tensor_multi = torch.from_numpy(y_test_multi).float()\n",
        "\n",
        "# Define the model\n",
        "class IrisMultiModel(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(IrisMultiModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 10)\n",
        "        self.fc2 = nn.Linear(10, 10)\n",
        "        self.fc3 = nn.Linear(10, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Get input and output dimensions\n",
        "input_dim = X_train_tensor_multi.shape[1]\n",
        "output_dim = y_train_tensor_multi.shape[1]\n",
        "\n",
        "# Instantiate the model, define loss function and optimizer\n",
        "model_multi = IrisMultiModel(input_dim, output_dim)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_multi.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    model_multi.train()\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model_multi(X_train_tensor_multi)\n",
        "    loss = loss_fn(outputs, y_train_tensor_multi)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluate the model\n",
        "model_multi.eval()\n",
        "with torch.no_grad():\n",
        "    test_outputs = model_multi(X_test_tensor_multi)\n",
        "    predicted_classes_multi = test_outputs.argmax(dim=1)\n",
        "    accuracy_multi = (predicted_classes_multi == y_test_tensor_multi.argmax(dim=1)).float().mean()\n",
        "    print(f'Test Accuracy: {accuracy_multi:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dU-JkE6BJevh",
        "outputId": "738095b1-cda1-40e3-f348-39194843444e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/100], Loss: 1.0523\n",
            "Epoch [20/100], Loss: 1.0192\n",
            "Epoch [30/100], Loss: 0.9825\n",
            "Epoch [40/100], Loss: 0.9421\n",
            "Epoch [50/100], Loss: 0.8975\n",
            "Epoch [60/100], Loss: 0.8482\n",
            "Epoch [70/100], Loss: 0.7956\n",
            "Epoch [80/100], Loss: 0.7416\n",
            "Epoch [90/100], Loss: 0.6880\n",
            "Epoch [100/100], Loss: 0.6361\n",
            "Test Accuracy: 0.8333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pytorch for House Price datasets"
      ],
      "metadata": {
        "id": "Fux-hzXdKHv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " #Load the House Prices dataset\n",
        "house_prices = pd.read_csv('train.csv')\n",
        "\n",
        "# Extract features and labels\n",
        "X = house_prices[['GrLivArea', 'YearBuilt']].values.astype(np.float32)\n",
        "y = house_prices['SalePrice'].values.astype(np.float32)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train_hp, X_test_hp, y_train_hp, y_test_hp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_hp = scaler.fit_transform(X_train_hp)\n",
        "X_test_hp = scaler.transform(X_test_hp)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_tensor_hp = torch.from_numpy(X_train_hp)\n",
        "y_train_tensor_hp = torch.from_numpy(y_train_hp)\n",
        "X_test_tensor_hp = torch.from_numpy(X_test_hp)\n",
        "y_test_tensor_hp = torch.from_numpy(y_test_hp)\n",
        "\n",
        "# Define the model\n",
        "class HousePriceModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(HousePriceModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(2, 64)\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.fc3 = nn.Linear(32, 1)  # Output layer for regression\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the model, define loss function and optimizer\n",
        "model_hp = HousePriceModel()\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = optim.Adam(model_hp.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    model_hp.train()\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model_hp(X_train_tensor_hp)\n",
        "    loss = loss_fn(outputs.squeeze(), y_train_tensor_hp)  # Squeeze to match dimensions\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# Evaluate the model\n",
        "model_hp.eval()\n",
        "with torch.no_grad():\n",
        "    test_outputs_hp = model_hp(X_test_tensor_hp)\n",
        "    test_loss = loss_fn(test_outputs_hp.squeeze(), y_test_tensor_hp)\n",
        "    print(f'Test Mean Squared Error: {test_loss:.2f}')\n",
        "\n",
        "# Make predictions\n",
        "predictions_hp = test_outputs_hp.squeeze().numpy()\n",
        "print(predictions_hp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJLyMYT5J4mO",
        "outputId": "45b4baa3-bb18-4941-ff22-9adfafe57965"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Mean Squared Error: 39648759808.00\n",
            "[11.201721  21.7411    14.451228   9.973671  14.625707  15.275073\n",
            " 11.599944   9.797759  15.304946   9.991077  11.763841  12.70478\n",
            " 14.729979  13.60302   13.62464   12.197957  14.592324  10.864821\n",
            " 15.338403  13.200096  10.140252  14.669813  13.611158  12.6871195\n",
            " 12.271913  11.479661  13.837986  13.687302  12.574628  13.620206\n",
            " 15.129973  16.999222  23.04221   12.538632  14.513419  10.665639\n",
            " 10.852935  15.962606  20.437073  13.188267  12.666505  16.16946\n",
            " 11.888495  17.840816  11.931513  18.958698  12.473018  11.967123\n",
            " 20.216808   9.620141  12.585076   9.981303   9.780431  23.104984\n",
            " 14.891927  14.877431  13.0765915 10.607633  10.636191  13.7635145\n",
            " 18.832825  10.359129  21.402485  14.345168  15.348565  12.776225\n",
            " 13.136816  19.088764  12.426297  12.327332  10.034683  10.17457\n",
            " 12.90165   12.186345  21.053951  11.745479  19.494267  19.258686\n",
            " 10.824586  13.604524  11.547257  14.886009  12.315021  14.509998\n",
            "  9.745408  10.84262   16.585823  10.408584  14.891927  13.773197\n",
            "  9.932973   9.698845  13.3731785 22.12542   10.267044  13.442588\n",
            " 12.026422  12.312772  13.888272  16.89016    9.865484  14.719138\n",
            " 14.70011   15.664735   9.767943   9.733516  11.290935  11.6641445\n",
            " 12.651897  12.78822   15.428143  11.193936  11.452767  12.64928\n",
            " 15.49223   15.072835  13.128044  12.645761  12.013818  13.858205\n",
            " 14.9604225 14.891927  12.554465  12.607207  10.562411  12.204963\n",
            " 18.561893  14.896537  12.143374  10.31822   13.818711  17.18158\n",
            " 13.917916  15.099139  17.601265  14.98898   15.306346  12.030297\n",
            " 13.797493  42.4236    23.891075  12.761365  11.597882  12.032219\n",
            " 11.683171  12.461442  12.321567  12.672367  13.734339  17.158768\n",
            " 13.961669   9.734842  17.093689  14.480355  14.661547  10.809599\n",
            "  9.970641  10.080711  18.660896  10.759137  13.951478  11.129248\n",
            " 14.776494   9.651177  11.235939  10.149321  15.164592  22.935131\n",
            " 26.024399  15.061764  16.094204  13.188267  11.920294  10.193107\n",
            " 24.498009   9.508977  10.242801  12.157151  11.191392  12.982419\n",
            " 10.049195  11.992098  11.8979025 10.458023  16.323784  12.530896\n",
            " 14.004195  13.894985  13.409995  10.792884   9.620553  14.801618\n",
            " 11.263555  10.650846  13.4544935 11.545884  13.524741  13.298996\n",
            " 13.777337  13.03205    9.750007  11.742226  13.149367  22.336075\n",
            " 13.006326  11.715829  15.466423  10.815389  10.434905  12.576431\n",
            " 16.936958   9.862335  10.459251  12.367102  10.113262  14.572736\n",
            " 13.668601  10.858456   9.979873  10.7492285 12.421152  13.638692\n",
            " 10.329529  12.332763  14.01071   10.554956  14.531056  14.927266\n",
            "  9.62424   16.543034  13.433564  11.392957  13.191503  10.585208\n",
            " 14.056741  13.557828  13.982435  12.380433  14.823192  12.266209\n",
            " 16.89167   23.26141   12.826829  27.043911  11.36357   14.4119625\n",
            "  9.647607  11.0687475 10.992021  14.612531  15.605438  11.028916\n",
            " 10.531618  17.915451  10.453531  10.955506  13.2233095  9.763843\n",
            " 11.736995  16.126202  16.877045  10.1831045 12.4608755 11.549168\n",
            " 15.240457  16.718094  12.024053  14.7788515 12.10785   12.471253\n",
            " 17.78209   11.222199  16.680733  10.6631975 10.337897  11.439858\n",
            " 14.526627  16.22559    9.498854  10.113592  14.580195  15.304946\n",
            " 14.195094  16.679874  15.19406    9.494732  14.895747  12.6940565\n",
            " 13.789943  13.560843  10.5226145 12.754394 ]\n"
          ]
        }
      ]
    }
  ]
}