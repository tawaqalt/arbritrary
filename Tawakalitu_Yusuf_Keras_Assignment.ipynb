{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPXhr8kEogqHmDYxkK0M5RX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tawaqalt/arbritrary/blob/master/Tawakalitu_Yusuf_Keras_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Problem 1] Sharing and executing the official tutorial model"
      ],
      "metadata": {
        "id": "wYvudrYplxV3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0trJmd6DjqBZ",
        "outputId": "3bb5e9e1-9714-4548-836f-6495d542b91a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.15.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(\"TensorFlow version:\", tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7FP5258xjs-v",
        "outputId": "453c7c5b-f4ef-4262-8413-0f681e67962c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPZ68wASog_I"
      },
      "source": [
        "## Build a machine learning model\n",
        "\n",
        "Build a `tf.keras.Sequential` model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3IKyzTCDNGo"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(10)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OeOrNdnkEEcR",
        "outputId": "6d637cb3-d44e-4a98-ccad-0f37da4fcabf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.40259278,  0.5827957 ,  0.10453623, -0.49031234, -0.13483012,\n",
              "        -0.31236565,  0.3927142 , -0.19655225, -0.32125467,  0.24674672]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "predictions = model(x_train[:1]).numpy()\n",
        "predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgjhDQGcIniO"
      },
      "source": [
        "The `tf.nn.softmax` function converts these logits to *probabilities* for each class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWSRnQ0WI5eq",
        "outputId": "ca4626e5-572b-4e0c-a951-7ee3282252e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.13693613, 0.16397558, 0.10164212, 0.05607049, 0.08000521,\n",
              "        0.06699086, 0.13559006, 0.07521643, 0.06639802, 0.117175  ]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "tf.nn.softmax(predictions).numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQyugpgRIyrA"
      },
      "source": [
        "Define a loss function for training using `losses.SparseCategoricalCrossentropy`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSkzdv8MD0tT"
      },
      "outputs": [],
      "source": [
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfR4MsSDU880"
      },
      "source": [
        "The loss function takes a vector of ground truth values and a vector of logits and returns a scalar loss for each example. This loss is equal to the negative log probability of the true class: The loss is zero if the model is sure of the correct class.\n",
        "\n",
        "This untrained model gives probabilities close to random (1/10 for each class), so the initial loss should be close to `-tf.math.log(1/10) ~= 2.3`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJWqEVrrJ7ZB",
        "outputId": "f86e6b31-2f56-405e-bd7b-8c75812ec440",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.7031991"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "loss_fn(y_train[:1], predictions).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9foNKHzTD2Vo"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=loss_fn,\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7dTAzgHDUh7",
        "outputId": "517deeac-252f-4459-8bed-c674f91b47d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 - 1s - loss: 0.0698 - accuracy: 0.9790 - 629ms/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.06983572244644165, 0.9789999723434448]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "model.evaluate(x_test,  y_test, verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYb6DrEH0GMv"
      },
      "outputs": [],
      "source": [
        "probability_model = tf.keras.Sequential([\n",
        "  model,\n",
        "  tf.keras.layers.Softmax()\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnqOZtUp1YR_",
        "outputId": "2407f600-af2c-4ee7-98da-6a2feabf07ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5, 10), dtype=float32, numpy=\n",
              "array([[2.90592972e-09, 1.83336935e-09, 1.15094053e-05, 6.73986142e-05,\n",
              "        6.06224749e-12, 4.76958064e-08, 1.76434144e-13, 9.99920726e-01,\n",
              "        5.84759192e-08, 2.56671797e-07],\n",
              "       [6.29487296e-10, 3.33659205e-04, 9.99662161e-01, 2.53803069e-06,\n",
              "        2.71335743e-13, 2.03424918e-07, 5.37389866e-09, 2.86747182e-14,\n",
              "        1.38465293e-06, 6.52231632e-13],\n",
              "       [1.71469722e-07, 9.99462426e-01, 7.17014191e-05, 9.68696804e-06,\n",
              "        1.65514975e-05, 5.61148170e-07, 8.35000355e-06, 3.23058048e-04,\n",
              "        1.07108914e-04, 3.98671403e-07],\n",
              "       [9.98885691e-01, 2.33585027e-08, 4.02420555e-05, 3.86875081e-06,\n",
              "        6.04595016e-06, 2.81772263e-05, 4.84819233e-04, 1.34536574e-04,\n",
              "        6.43496833e-07, 4.16018796e-04],\n",
              "       [5.48915250e-06, 1.58233817e-07, 3.47472778e-05, 2.52011796e-06,\n",
              "        9.87487555e-01, 2.71242379e-06, 2.94105703e-05, 4.03273480e-05,\n",
              "        3.44278328e-06, 1.23936189e-02]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "probability_model(x_test[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Problem 3] Learning Iris (binary classification) with Keras"
      ],
      "metadata": {
        "id": "F_TRvfSRxU5R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = pd.read_csv('Iris.csv')\n",
        "\n",
        "# Filter for Iris-versicolor and Iris-virginica\n",
        "iris_binary = iris[(iris['Species'] == 'Iris-versicolor') | (iris['Species'] == 'Iris-virginica')]\n",
        "\n",
        "# Extract features and labels\n",
        "X = iris_binary.drop('Species', axis=1)\n",
        "y = iris_binary['Species']\n",
        "\n",
        "# Encode labels (0 for Iris-versicolor, 1 for Iris-virginica)\n",
        "y = np.where(y == 'Iris-versicolor', 0, 1)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train_binary, X_test_binary, y_train_binary, y_test_binary = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_binary = scaler.fit_transform(X_train_binary)\n",
        "X_test_binary = scaler.transform(X_test_binary)\n",
        "\n",
        "# Define the model\n",
        "model_binary = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(10, activation='relu', input_shape=(X_train_binary.shape[1],)),\n",
        "    tf.keras.layers.Dense(10, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
        "model_binary.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model_binary.fit(X_train_binary, y_train_binary, epochs=5, batch_size=5, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model_binary.evaluate(X_test_binary, y_test_binary)\n",
        "print(f'Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}')\n",
        "\n",
        "# Make predictions\n",
        "predictions_binary = model_binary.predict(X_test_binary)\n",
        "predicted_classes_binary = (predictions_binary > 0.5).astype(int)\n",
        "print(predicted_classes_binary.flatten())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsnTj0Z5xUeH",
        "outputId": "9a030d79-402b-42d1-e2db-60d09e798ee5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "13/13 [==============================] - 1s 18ms/step - loss: 0.6786 - accuracy: 0.5312 - val_loss: 0.7021 - val_accuracy: 0.4375\n",
            "Epoch 2/5\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.6413 - accuracy: 0.5312 - val_loss: 0.6708 - val_accuracy: 0.5000\n",
            "Epoch 3/5\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.6044 - accuracy: 0.5781 - val_loss: 0.6437 - val_accuracy: 0.5000\n",
            "Epoch 4/5\n",
            "13/13 [==============================] - 0s 6ms/step - loss: 0.5735 - accuracy: 0.6875 - val_loss: 0.6186 - val_accuracy: 0.6250\n",
            "Epoch 5/5\n",
            "13/13 [==============================] - 0s 5ms/step - loss: 0.5430 - accuracy: 0.7969 - val_loss: 0.5939 - val_accuracy: 0.8125\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.4206 - accuracy: 0.9000\n",
            "Test Loss: 0.4206, Test Accuracy: 0.9000\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "[1 1 1 0 0 0 0 1 0 1 0 0 1 0 0 0 1 1 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Problem 4] Learning Iris (multi-level classification) with Keras"
      ],
      "metadata": {
        "id": "LCYEUxi40fKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Iris dataset\n",
        "iris = pd.read_csv('Iris.csv')\n",
        "\n",
        "# Extract features and labels\n",
        "X = iris.drop('Species', axis=1)\n",
        "y = iris['Species']\n",
        "\n",
        "# One-hot encode the labels\n",
        "encoder = OneHotEncoder()\n",
        "y_encoded = encoder.fit_transform(y.values.reshape(-1, 1)).toarray()\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train_multi, X_test_multi, y_train_multi, y_test_multi = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_multi = scaler.fit_transform(X_train_multi)\n",
        "X_test_multi = scaler.transform(X_test_multi)\n",
        "\n",
        "# Define the model\n",
        "model_multi = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(10, activation='relu', input_shape=(X_train_multi.shape[1],)),\n",
        "    tf.keras.layers.Dense(10, activation='relu'),\n",
        "    tf.keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
        "model_multi.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model_multi.fit(X_train_multi, y_train_multi, epochs=10, batch_size=5, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model_multi.evaluate(X_test_multi, y_test_multi)\n",
        "print(f'\\nTest Loss: {loss:.4f}, \\nTest Accuracy: {accuracy:.4f}')\n",
        "\n",
        "# Make predictions\n",
        "predictions_multi = model_multi.predict(X_test_multi)\n",
        "predicted_classes_multi = np.argmax(predictions_multi, axis=1)\n",
        "print(predicted_classes_multi)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ec8KOm-F0qUC",
        "outputId": "b108bbe4-11f1-4c38-f4a1-aaf9647e3f00"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "20/20 [==============================] - 1s 12ms/step - loss: 1.0888 - accuracy: 0.2812 - val_loss: 1.0956 - val_accuracy: 0.1667\n",
            "Epoch 2/10\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 1.0005 - accuracy: 0.3438 - val_loss: 1.0286 - val_accuracy: 0.2500\n",
            "Epoch 3/10\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.9251 - accuracy: 0.5104 - val_loss: 0.9726 - val_accuracy: 0.2917\n",
            "Epoch 4/10\n",
            "20/20 [==============================] - 0s 4ms/step - loss: 0.8581 - accuracy: 0.6250 - val_loss: 0.9236 - val_accuracy: 0.4583\n",
            "Epoch 5/10\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.8030 - accuracy: 0.6458 - val_loss: 0.8818 - val_accuracy: 0.5417\n",
            "Epoch 6/10\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.7556 - accuracy: 0.7188 - val_loss: 0.8427 - val_accuracy: 0.8750\n",
            "Epoch 7/10\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.7133 - accuracy: 0.8125 - val_loss: 0.8043 - val_accuracy: 0.9167\n",
            "Epoch 8/10\n",
            "20/20 [==============================] - 0s 6ms/step - loss: 0.6753 - accuracy: 0.8542 - val_loss: 0.7676 - val_accuracy: 0.9167\n",
            "Epoch 9/10\n",
            "20/20 [==============================] - 0s 5ms/step - loss: 0.6386 - accuracy: 0.9167 - val_loss: 0.7215 - val_accuracy: 0.9167\n",
            "Epoch 10/10\n",
            "20/20 [==============================] - 0s 7ms/step - loss: 0.5998 - accuracy: 0.9271 - val_loss: 0.6794 - val_accuracy: 0.9167\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.5769 - accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7a935765be20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Loss: 0.5769, \n",
            "Test Accuracy: 1.0000\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Problem 5] Learning House Prices with Keras"
      ],
      "metadata": {
        "id": "FpGksF9K05b8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "\n",
        "# Load the House Prices dataset\n",
        "house_prices = pd.read_csv('train.csv')\n",
        "\n",
        "# Extract features and labels\n",
        "X = house_prices[['GrLivArea', 'YearBuilt']]\n",
        "y = house_prices['SalePrice']\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train_hp, X_test_hp, y_train_hp, y_test_hp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_hp = scaler.fit_transform(X_train_hp)\n",
        "X_test_hp = scaler.transform(X_test_hp)\n",
        "\n",
        "# Define the model\n",
        "model_hp = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train_hp.shape[1],)),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(1)  # Output layer for regression\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "model_hp.compile(optimizer='adam', loss=loss_fn, metrics=['MeanAbsoluteError'])\n",
        "\n",
        "# Train the model\n",
        "model_hp.fit(X_train_hp, y_train_hp, epochs=5, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, mae = model_hp.evaluate(X_test_hp, y_test_hp)\n",
        "print(f'\\nTest Mean Absolute Error: ${mae:.2f}')\n",
        "\n",
        "# Make predictions\n",
        "predictions_hp = model_hp.predict(X_test_hp)\n",
        "print(predictions_hp.flatten())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkuVwIqL00Rb",
        "outputId": "d3e67e3a-fa5f-461d-f5d6-9f652d7b8927"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "30/30 [==============================] - 1s 8ms/step - loss: 39146848256.0000 - mean_absolute_error: 181526.1562 - val_loss: 37841874944.0000 - val_mean_absolute_error: 181101.3438\n",
            "Epoch 2/5\n",
            "30/30 [==============================] - 0s 4ms/step - loss: 39146401792.0000 - mean_absolute_error: 181524.9219 - val_loss: 37841203200.0000 - val_mean_absolute_error: 181099.5312\n",
            "Epoch 3/5\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 39145373696.0000 - mean_absolute_error: 181522.1250 - val_loss: 37839720448.0000 - val_mean_absolute_error: 181095.4531\n",
            "Epoch 4/5\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 39143251968.0000 - mean_absolute_error: 181516.3281 - val_loss: 37836787712.0000 - val_mean_absolute_error: 181087.5625\n",
            "Epoch 5/5\n",
            "30/30 [==============================] - 0s 3ms/step - loss: 39139323904.0000 - mean_absolute_error: 181505.7812 - val_loss: 37831716864.0000 - val_mean_absolute_error: 181073.8438\n",
            "10/10 [==============================] - 0s 2ms/step - loss: 39643316224.0000 - mean_absolute_error: 178810.6719\n",
            "\n",
            "Test Mean Absolute Error: $178810.67\n",
            "10/10 [==============================] - 0s 2ms/step\n",
            "[26.938606 37.01614  34.688683 18.252855 31.341293 40.047665 20.462116\n",
            " 20.91758  39.982426 19.385258 26.459839 32.64978  35.836063 29.698816\n",
            " 28.047802 29.2512   30.143398 25.290707 36.976315 29.289917 16.721504\n",
            " 31.79746  30.284737 30.843645 27.518116 28.250221 31.821861 33.492783\n",
            " 26.284565 31.269665 37.098213 31.324278 36.264572 30.38889  30.860386\n",
            " 25.056965 17.941254 30.594006 34.81364  32.40018  29.533258 28.12185\n",
            " 30.551304 33.598835 29.239565 38.004745 31.425556 30.213558 35.75198\n",
            " 17.483065 30.287716 19.572895 22.437664 37.651043 38.14543  30.690662\n",
            " 25.036415 26.393497 26.502909 34.075066 47.223667 22.604923 37.274563\n",
            " 31.726273 31.33857  19.937897 31.328545 33.41676  32.327576 27.394926\n",
            " 20.21536  23.331469 31.641441 28.951065 36.955326 27.795422 33.718807\n",
            " 32.98315  24.962112 35.59235  29.589928 35.724655 29.648447 35.31249\n",
            " 21.537172 22.711355 32.076576 22.526764 38.14543  30.83331  18.450249\n",
            " 17.88375  35.375088 36.957203 16.5038   31.031427 26.684532 27.415144\n",
            " 30.733116 28.228214 17.63897  29.818995 24.08379  38.256245 21.178078\n",
            " 17.862625 28.27792  21.300705 32.506516 20.705643 37.989456 28.776703\n",
            " 27.735579 28.992393 31.414057 37.12499  32.046936 32.822254 19.748505\n",
            " 29.330282 38.98771  38.14543  26.27769  24.912624 23.614292 30.765789\n",
            " 32.243702 36.117176 30.617157 18.405922 29.746609 32.108345 27.429312\n",
            " 28.21288  44.981487 31.130089 32.066456 31.243176 28.912281 69.38466\n",
            " 40.56799  31.071268 21.252544 25.277376 27.659082 31.840517 21.57894\n",
            " 28.459522 35.94878  41.758915 33.233738 20.505669 26.811115 22.791567\n",
            " 35.09895  17.579807 22.369886 19.9384   46.54754  24.985455 32.63606\n",
            " 28.567352 31.392815 17.433216 25.344923 16.529076 27.931606 39.987072\n",
            " 44.66014  23.688923 30.679482 32.40018  26.602234 20.15565  41.26244\n",
            " 17.067635 20.304115 23.339928 24.722248 27.707256 22.05322  28.451065\n",
            " 30.904795 21.694841 30.462425 26.66265  29.038086 30.72916  30.69421\n",
            " 26.541672 20.941181 35.486217 25.09535  26.598309 31.1052   18.878674\n",
            " 31.676208 31.74622  27.726992 30.985504 19.662174 26.085754 31.333393\n",
            " 38.28701  29.955856 29.51605  27.654608 25.501099 20.642742 32.044075\n",
            " 28.943703 22.941166 23.115011 27.36462  20.222435 31.75505  31.011152\n",
            " 25.109882 20.095482 18.269321 32.400692 31.437622 18.743233 28.81587\n",
            " 31.300829 23.429403 34.808903 36.089695 17.37172  41.135105 32.60214\n",
            " 24.507969 32.82475  23.234802 25.490572 32.377796 32.91466  31.959263\n",
            " 25.25534  32.212948 41.79971  37.191624 24.884125 46.3718   18.29784\n",
            " 34.919315 18.95198  27.76918  28.023445 36.045616 31.087017 23.8474\n",
            " 21.774467 44.337734 24.589308 27.888306 31.111504 21.850395 23.912317\n",
            " 28.304012 32.258213 24.355867 32.570534 22.595722 31.655785 32.519123\n",
            " 27.859486 37.912067 29.041853 27.986282 32.900043 26.160618 31.772926\n",
            " 26.631622 22.99519  29.022423 30.103859 32.20864  19.334255 19.36099\n",
            " 29.35081  39.982426 30.754364 40.94244  31.627031 19.160591 29.174686\n",
            " 29.806648 30.196178 30.340048 21.70839  33.02227 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Problem 6] Learning MNIST with Keras"
      ],
      "metadata": {
        "id": "0orYQi-15exC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(X_train_mnist, y_train_mnist), (X_test_mnist, y_test_mnist) = mnist.load_data()\n",
        "\n",
        "# Normalize the images\n",
        "X_train_mnist = X_train_mnist / 255.0\n",
        "X_test_mnist = X_test_mnist / 255.0\n",
        "\n",
        "# Reshape the images\n",
        "X_train_mnist = X_train_mnist.reshape(X_train_mnist.shape[0], 28, 28, 1)\n",
        "X_test_mnist = X_test_mnist.reshape(X_test_mnist.shape[0], 28, 28, 1)\n",
        "\n",
        "# Define the model\n",
        "model_mnist = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(10)  # Output layer for 10 classes\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model_mnist.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model_mnist.fit(X_train_mnist, y_train_mnist, epochs=5, batch_size=32, validation_data=(X_test_mnist, y_test_mnist))\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model_mnist.evaluate(X_test_mnist, y_test_mnist)\n",
        "print(f'\\nTest Loss: {loss:.4f}, \\nTest Accuracy: {accuracy:.4f}')\n",
        "\n",
        "# Make predictions\n",
        "predictions_mnist = model_mnist.predict(X_test_mnist)\n",
        "predicted_classes_mnist = np.argmax(predictions_mnist, axis=1)\n",
        "print(predicted_classes_mnist)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_OpZMO51czb",
        "outputId": "4cc3ea52-5461-47c1-fc6e-3b862b970ab7"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 61s 32ms/step - loss: 0.1482 - accuracy: 0.9541 - val_loss: 0.0524 - val_accuracy: 0.9827\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 59s 31ms/step - loss: 0.0478 - accuracy: 0.9854 - val_loss: 0.0508 - val_accuracy: 0.9828\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 59s 31ms/step - loss: 0.0340 - accuracy: 0.9891 - val_loss: 0.0289 - val_accuracy: 0.9908\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 57s 30ms/step - loss: 0.0254 - accuracy: 0.9921 - val_loss: 0.0678 - val_accuracy: 0.9794\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 56s 30ms/step - loss: 0.0215 - accuracy: 0.9931 - val_loss: 0.0272 - val_accuracy: 0.9915\n",
            "313/313 [==============================] - 3s 9ms/step - loss: 0.0272 - accuracy: 0.9915\n",
            "\n",
            "Test Loss: 0.0272, \n",
            "Test Accuracy: 0.9915\n",
            "313/313 [==============================] - 4s 13ms/step\n",
            "[7 2 1 ... 4 5 6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Problem 7] Rewriting to PyTorch\n",
        "## 1. Iris Binary Classification with PyTorch"
      ],
      "metadata": {
        "id": "AnbXA7aE7El-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = pd.read_csv('Iris.csv')\n",
        "\n",
        "# Filter for Iris-versicolor and Iris-virginica\n",
        "iris_binary = iris[(iris['Species'] == 'Iris-versicolor') | (iris['Species'] == 'Iris-virginica')]\n",
        "\n",
        "# Extract features and labels\n",
        "X = iris_binary.drop('Species', axis=1).values\n",
        "y = np.where(iris_binary['Species'] == 'Iris-versicolor', 0, 1)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train_binary, X_test_binary, y_train_binary, y_test_binary = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_binary = scaler.fit_transform(X_train_binary)\n",
        "X_test_binary = scaler.transform(X_test_binary)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.FloatTensor(X_train_binary)\n",
        "y_train_tensor = torch.FloatTensor(y_train_binary).unsqueeze(1)  # Add a dimension for binary classification\n",
        "X_test_tensor = torch.FloatTensor(X_test_binary)\n",
        "y_test_tensor = torch.FloatTensor(y_test_binary).unsqueeze(1)\n",
        "\n",
        "# Define the model\n",
        "class IrisBinaryModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(IrisBinaryModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(4, 10)\n",
        "        self.fc2 = nn.Linear(10, 10)\n",
        "        self.fc3 = nn.Linear(10, 1)  # Output layer for binary classification\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.sigmoid(self.fc3(x))\n",
        "        return x\n",
        "\n",
        "# Instantiate the model, define loss function and optimizer\n",
        "model_binary = IrisBinaryModel()\n",
        "loss_fn = nn.BCELoss()\n",
        "optimizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8a80679-58d3-48b6-ec12-018f022eaac6",
        "id": "ZIR4Z0Ci7auG"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Adam (\n",
              "Parameter Group 0\n",
              "    amsgrad: False\n",
              "    betas: (0.9, 0.999)\n",
              "    capturable: False\n",
              "    differentiable: False\n",
              "    eps: 1e-08\n",
              "    foreach: None\n",
              "    fused: None\n",
              "    lr: 0.001\n",
              "    maximize: False\n",
              "    weight_decay: 0\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    }
  ]
}